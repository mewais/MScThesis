%% The following is a directive for TeXShop to indicate the main file
%%!TEX root = diss.tex

\chapter{Design}
\label{ch:Design}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% Naiive Implementation
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Straightforward Implementation}
\label{sec:Straightforward Implementation}
In this section we propose an implementation of a cache that combines inter-line and intra-line compression techniques, this implementation is built simply by combining \hyperref[sec:Dedup]{Dedup} and \hyperref[sec:BDI]{BDI}, this cache is able to perform deduplication on BDI compressed blocks and uses the same infrastructure and replacement policy to \hyperref[sec:Dedup]{Dedup}.

\subsection{Structure}
\label{ssec:DedupBDIStructure}
\begin{figure}
    \includegraphics[width=\textwidth]{DedupBDI.pdf}
    \label{fig:DedupBDI}
    \caption{General structure of a DedupBDI Cache, tags that have the same data line are organized together in a linked list using previous and next pointers (PPtr \& NPtr) and they all point to their compressed data line using DPtr and save it's compression data, the compressed data line in return only points to the head tag of the linked list using TPtr and keeps count of how many tags are associated with it using TCount, the hashes in the hash array point to corresponding compressed data lines, if any.}
\end{figure}
The cache consists of three arrays: a tag array, a data array, and a hash array. Unlike a normal cache and similar to a \hyperref[sec:Dedup]{Dedup} cache, the tag and data arrays are decoupled and do not have a one to one mapping. On top of it's normal operation, the tag array is also used to save compression and deduplication metadata. The general structure of the cache is shown in Figure \ref{fig:DedupBDI}.
\subsubsection{Tag Array}
\label{sssec:DedupBDITag}
The DedupBDI tag array is almost the same as the tag array of \hyperref[sssec:DedupTag]{Dedup} cache. It's a normal set associative array with no limitations on its associativity or organization. Because the tag and data arrays are decoupled, a tag entry needs an extra pointer to its corresponding data entry, unlike \hyperref[sssec:DedupTag]{Dedup tag array} data pointer, this is actually divided into two pointers: a data set pointer, and a segment pointer, allowing the tag to point to a compressed line anywhere withing the data array boundaries. The tag entries also need to save compression encoding to be able to resolve the size of the compressed data it points to. Similarly to \hyperref[sssec:DedupTag]{Dedup}, a tag entry has two extra previous/next pointers to allow a group of tags with a similar data line to form a linked list, those pointers need to be big enough to point to tag sets, since reading a tag requires reading the whole set, the required tag from that set can be resolved by comparing it's data pointer. The use of a linked list is necessary in case of data line eviction. The tag array must have more tag entries than the data array, otherwise it wouldn't be able to utilize compression and deduplication.
\subsubsection{Data Array}
\label{sssec:DedupBDIData}
The DedupBDI data array is similar to that of a \hyperref[sssec:BDIData]{BDI} cache, each data line is logically divided into segments of eight bytes each, a data line compressed can occupy any number of segments between one and eight (assuming a 64 byte cache line). Accounting for the worst case scenario where each line is compressed into one segment, each segment must have two metadata fields: a pointer to the head of the tag linked list that's associated with this line, and a counter of the tags in that linked list. Both those fields are useful in case an eviction of a data line happens. The pointer needs to be big enough to only point to a tag set, the target tag from this set can be known by comparing its data pointer. Te counter only needs to be two bits large, it represents zero/one/two/more, in cases of eviction it can be determined if the count is less than 3 by checking only one tag entry for previous and next pointers.
\subsubsection{Hash Array}
\label{sssec:DedupBDIHash}
The hash array is the same as that of a \hyperref[sssec:DedupHash]{Dedup} cache, it's a set associative structure, when a data line is hashed the hash is split into two parts, one is used to index the hash array, while the other is saved in the hash array itself, similar to the tag part of the address being saved in data array. With each entry in the hash array there's also a pointer to a data line that meets this hash, the pointer is actually two parts, one points to the data set, and the other to the segment in that set. The hash itself is computed like dedup, through an XOR tree, based on experiments done in \cite{tian2014last} this is enough to keep hash collisions less than 1%.

\subsection{Operations}
\label{ssec:DedupBDIOperations}
\subsubsection{Tag Miss}
On a tag miss, just like a normal cache, a request for the missed address is forwarded to the next cache level. Meanwhile a victim tag line has to be selected according to the tag replacement policy (e.g. LRU) and evicted if necessary. This eviction may or may not trigger a data line eviction, depending on whether the tag was single (no deduplication) or part of a linked list (deduplication), if the tag is part of linked list then the tags before and after it have to be fixed (i.e. have to point to each other instead of pointing to the victim).

Once the requested line is recieved, it can be used to service the miss request right away, then placement of the new line can start off the critical path. The recieved line is first hashed and BDI compressed, the hash is used to compare against the existing hash array, if it hits then the received line has to be compared against the line pointed to by the hash entry, this is needed to make sure because the lines could be different but their hashes are similar (i.e. hash collision). There are four possible outcomes for this scenario, similar to \hyperref[sssec:DedupOperations]{Dedup}:
\begin{itemize}
    \item \textbf{Hash Miss:} No similar hash is found, either because similar lines do not exist in the data array, or because the hash array is not big enough to keep track of all data lines. In this case we use the data replacement policy to evict victim data line(s) until a sufficient area for the compressed line exists. The new tag is inserted and the tag and data lines have to point to each other, the tag will not point to other tags and will not create a linked list because no deduplication is happening yet. A new hash entry will be selected based on the hash replacement policy and will point to the newly inserted data line.
    \item \textbf{Hash Hit, Line Similar:} In which case the received data line can be deduplicated, it will use the same data line and hash entry. Only the new tag needs to be inserted, it's inserted as the head of the linked list and points to the deduplicated data line. The data line's tag counter (deduplication counter) has to be incremented and it has to pint to the new linked list head.
    \item \textbf{Hash Hit, Line Invalid:} Because hashes point to data lines, but data lines do not point to hashes, when a data line is evicted, it's associated hash might not, causing a situation like this to arise. In this case, we utilize the invalid space right away instead of consulting the data replacement policy. This is similar to the same case in \hyperref[sssec:DedupOperations]{Dedup} with one minor difference, if the invalid space is not enough for the compressed line, we use the data replacement policy to start evicting from the current data set until a sufficient area is enough for the compressed data line to be inserted. The tag entry is also inserted, it has to point to the data line, but it doesn't point to any other tags because the data line is not deduplicated yet and shouldn't have a linked list associated with it. The data line in return also has to point to the tag entry, the hash entry is not changed because it already points to the space we used for the dataline.
    \item \textbf{Hash Hit, Line Different:} This case happens only on a hash collision, once a collision happens, it's treated like a hash miss with one modification, a new hash insertion is not needed, the same hash entry will be changed to point to the newly inserted data line in only if the line it was previously pointing to was not deduplicated (i.e. its tag counter is 1), otherwise it remains untouched. This follows the same policy used in \hyperref[sssec:DedupOperations]{Dedup}.
\end{itemize}
\subsubsection{Tag Hit}
A tag hit has to be treated differently depending on whether it's a read or write request, a read hit is simple, after accessing the tag, the tag's data pointer can be used to access the data line and service the request, a write hit however can be complicated because it can cause the data line to change and thus change it's compressed size and/or hash. If the line was distinct (i.e. had a tag counter of 1) and the new written size is the same or less than the original size then it can be overwritten right away with the tag updated accordingly, otherwise, the tag disconnects from the data line causing its eviction if it's not deduplicated or causing its counter to decrement and then the request is treated as a tag miss and can follow one of the four cases mentioned above.

\subsection{Replacement Policies}
\label{ssec:Replacement Policies}
Some parts of the DedupBDI cache like the tag array can still operate in the same way with the same replacement policies, the tag array can use one of the known replacement policies like LRU. Other parts of the cache have to be treated differently, since the data and hash array are decoupled from the tag array each one of them needs it's own replacement policy.
\subsubsection{Data Array}
\label{sssec:DedupBDIData}
The data array uses a somewhat similar replacement policy to \hyperref[sssec:DedupDataRepl]{Dedup data replacement policy}, a free list keeping track of free data sets (not lines) is used, whenever needed an entry of this list was used to insert a line. If the list if empty then up to four data sets are picked at random, if any of the sets have enough segments for insertion of the line then it's selected right away, otherwise the one with the least sum of deduplication counters is picked. The picked data set then is used for the second part of the replacement policy, in which a segment (or group of segments representing a compressed data line) with the lowest deduplication counter is selected for eviction, segments keep evicted as necessary until a large enough space is freed for the insertion. Each segment or group of segments evicted can trigger a chain of evictions for the tag linked list associated with it.
\subsubsection{Hash Array}
\label{sssec:DedupBDIHash}
The hash array is indexed by a part of the hash, the selection of a victim hash set is thus dependant on the hash that needs to be inserted, once that set is selected, a hash entry is then selected based on LRU.

\section{Establishing an Upper Bound}
\label{sec:Upper Bound}
To be able to evaluate the strait forward design we discussed in the previous section, an upper bound must be established, we try to get the upper bound of enhancements by idealizing the design choices we have selected. There are two aspects we can idealize in this design:
\begin{itemize}
    \item \textbf{Finding a similar line:} Finding a similar line in the DedupBDI cache has two sources of possible error: the first is the hashing itself, because hashing data lines to a smaller space means it will never be perfect and collisions are inevitable, the second limitation is the size of hash array, because the number of entries in the hash array is less than that of the data array, there's a chance that an opportunity for deduplication is missed because the hash array was too small and had to evict its hash. Finding a similar line can be idealized by directly searching through the data array and comparing each line until a similar data line is found.
    \item \textbf{Replacement of data lines:} Imprefections in the replacement policy of data array comes from two sources: the first is the free list, since we cannot make the free list keep track of all free segments in the data array because it would be unpractically large, it can only keep track of free sets, that means even if only one segment is used from a set that set is not kept in the free list anymore. the second source is the random selection of data lines whenever the free list is empty, our experiments show that the random selection causes utilization of the data array to be less than a 100\% as shown in \ref{ch:Results}, selecting 4 random sets might now always yield a set with enough free segments, selecting more than 4 can be time inefficient. We idealize the replacement policy by directly searching the whole data cache for free segments that are enough to insert a new compressed line, this ensures a 100\% utilization of the data array, if no free segments are available we use data array wide LRU to pick a victim segment(s) instead of doing it randomly.
\end{itemize}
with the use of perfect deduplication and data insertion, a hash array is no longer needed, and the tag array is no different than a normal cache.

\section{Final Implementation}
\label{sec:Final Implementation}
