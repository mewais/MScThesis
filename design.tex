%% The following is a directive for TeXShop to indicate the main file
%%!TEX root = diss.tex

\chapter{Design}
\label{ch:Design}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% Naive Implementation
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Straightforward Implementation}
\label{sec:Straightforward Implementation}
In this section we propose an implementation of a cache that combines inter-line and intra-line compression techniques, this implementation is built simply by combining \hyperref[sec:Dedup]{Dedup} and \hyperref[sec:BDI]{BDI}, this cache is thus able to perform deduplication on BDI compressed blocks and uses the same infrastructure and a similar replacement policy to \hyperref[sec:Dedup]{Dedup}.

\subsection{Structure}
\label{ssec:DedupBDIStructure}
\begin{figure}
    \makebox[\textwidth][c]{\includegraphics[width=1.5\textwidth]{BDIvsDedupvsDedupBDI_Data.pdf}}
    \label{fig:DedupBDI_Data}
    \caption[DedupBDI Data Array]{The figure shows three different data lines from the BDI, Dedup, and DedupBDI Caches. A data line in a BDI cache can be logically divided into 8 segments, a compressed data line can be represented by one to eight segments, and the data line doesn't hold any metadata. A data line in Dedup cache can only hold uncompressed data lines, so it needs its full 64 byte size, also since it's decoupled from its tag array it needs a tag pointer to be able to point to the tags associated with it, and a counter for its deduplication. A data line in a DedupBDI cache is a combination of both, it needs to be able to hold deduplicated compressed lines, so like a BDI data line it's also divided into 8 segments, but since each segment can hold a compressed line on its own, each segment must have space for the same metadata as a Dedup data line. If a group of segments represent a compressed line, only the metadata of the fist segment in the group is used.}
\end{figure}
\begin{figure}
    \includegraphics[width=\textwidth]{BDIvsDedupvsDedupBDI_Tag.pdf}
    \label{fig:DedupBDI_Tag}
    \caption[DedupBDI Tag Array]{The figure shows three tag lines from the BDI, Dedup, and DedupBDI Caches. In a BDI cache, tags and data sets are coupled, so each tag needs to know how big is the comressed data associated with it and where it begins, each tag has an extra metadata field that encodes the compression and its size. A Dedup cache allows multiple tags to have the same data line, those tags must be arranged in a linked list so each tag has a previous and a next pointer, all tags have a data pointer to point to their respective data line. In DedupBDI a tag entry must be able to allow deduplication as long as deal with compressed data lines, thus it has the same metadata as a tag entry of a BDI and Dedup cache. The data pointer in DedupBDI must be wider to allow a granularity of data segments not just data lines.}
\end{figure}
\begin{figure}
    \makebox[\textwidth][c]{\includegraphics[width=1.5\textwidth]{DedupBDI.pdf}}
    \label{fig:DedupBDI}
    \caption[DedupBDI Cache]{General structure of a DedupBDI Cache, tags that have the same data line are organized together in a linked list using previous and next pointers (PPtr \& NPtr) and they all point to their compressed data line using DPtr and save it's compression data, the compressed data line in return only points to the head tag of the linked list using TPtr and keeps count of how many tags are associated with it using TCount, the hashes in the hash array point to corresponding compressed data lines, if any.}
\end{figure}
The cache consists of three arrays:
%\begin{itemize}
%    \item \textbf{Tag Array: } is a normal tag array, in addition to saving compression and deduplication metadata. Decoupled from the data array, tags connect to data lines through pointers.
%    \item \textbf{Data Array: } is a normal data array that must be able to handle compressed data lines, data lines connect to their tags through pointers.
%    \item \textbf{Hash Array: } used to save hashes of data lines saved in the data array, the hashes facilitate finding similar lines and deduplication
%\end{itemize}
a tag array, a data array, and a hash array. Unlike a normal cache and similar to a \hyperref[sec:Dedup]{Dedup} cache, the tag and data arrays are decoupled and do not have a one to one mapping, instead related tags and data entries have to be able to point to each other. On top of it's normal operation, the tag array is also used to save compression and deduplication metadata. The hash array is needed to facilitate searching for similar data lines to enable deduplication.

The general structure of the cache is shown in Figure \ref{fig:DedupBDI}.
\subsubsection{Tag Array}
\label{sssec:DedupBDITag}
The DedupBDI tag array is a normal, set associative array with no limitations on its associativity or organization. It incorporates extra features from \hyperref[sssec:DedupTag]{Dedup} and \hyperref[sssec:BDITag]{BDI} to support dealing with deduplicated and compressed data lines. Deduplication in its essence means that multiple tags that have similar data lines will be allowed to only save one copy of this data, this breaks the conventional one to one relationship between tags and data in a normal cache making the tag and data caches decoupled. Because of this, a tag entry needs an extra pointer to its corresponding data entry, unlike \hyperref[sssec:DedupTag]{Dedup tag array}'s data pointer, this pointer needs to be able to point to compressed data lines that are smaller in size than normal cache lines, the pointer essentially needs to be larger in size to be able to address lines of smaller granularity. It is actually divided into two pointers: a data set pointer, and a segment pointer, allowing the tag to point to a compressed line anywhere withing the data array boundaries. The tag entries also need to save compression encoding to be able to resolve the size of the compressed data it points to. Similarly to \hyperref[sssec:DedupTag]{Dedup}, tag entries that share the same deduplicated data line are arranged in a linked list, facilitating their eviction in case that data line is evicted. So each tag entry has two extra previous/next pointers to allow them to form a linked list, those pointers need to be big enough to point to tag sets, since reading a tag requires reading the whole set, the required tag from that set can be resolved by comparing it's data pointer. The use of a linked list is necessary in case of data line eviction. In general, the tag array must have more tag entries than the data array, otherwise it wouldn't be able to utilize compression and deduplication. The structure and difference between tag entries in Dedup, BDI, and DedupBDI is shown in Figure \ref{fig:DedupBDI_Tag}
\subsubsection{Data Array}
\label{sssec:DedupBDIData}
The DedupBDI data array is similar to that of a \hyperref[sssec:BDIData]{BDI} cache, each data line in the DedupBDI data array is logically divided into eight segments of eight bytes each (assuming a 64 byte cache line), a compressed data line can occupy any number of segments between one and eight, in the best case scenario all lines will be compressed into one segment each. The data array is decoupled from the tag array because it must also support deduplication, In case a compressed data line is evicted, all the tags associated with it must also be evicted, the compressed data line then must be able to point to the tag linked list and to know how long that list is. Accounting for the best case scenario where each line is compressed into one segment, each segment must have two metadata fields: a pointer to the head of the tag linked list that's associated with this line, and a counter of the tags in that linked list. Both those fields are useful in case an eviction of a data line happens. The pointer needs to be big enough to only point to a tag set, the target tag from this set can be known by comparing its data pointer. The counter only needs to be two bits large, it represents zero/one/two/more, in cases of eviction it can be determined if the count is less than 3 by checking only one tag entry for previous and next pointers. The structure and difference between tag entries in Dedup, BDI, and DedupBDI is shown in Figure \ref{fig:DedupBDI_Data}
\subsubsection{Hash Array}
\label{sssec:DedupBDIHash}
The hash array is a simple set associative structure, it's used to store hashes of some of the data lines providing a way to facilitate finding similar lines and deduplicate them. It is the same as that of a \hyperref[sssec:DedupHash]{Dedup} cache. When a data line is hashed the hash is split into two parts, one is used to index the hash array, while the other is saved in the hash array itself, similar to the tag part of the address being saved in data array. With each entry in the hash array there's also a pointer to a data line that meets this hash, just like the data pointers in the tag array, the pointer is actually two parts, one points to the data set, and the other to the segment in that set. The hash itself is computed like \hyperref[sssec:DedupHash]{Dedup}, through an XOR tree, based on experiments done in \cite{dedup} this is enough to keep hash collisions less than 1%.

\subsection{Operations}
\label{ssec:DedupBDIOperations}
\begin{figure}
    \includegraphics[width=\textwidth,height=\textheight]{DedupBDI_Read.pdf}
    \label{fig:DedupBDI_Read}
    \caption[DedupBDI Read]{Read access flowchart.}
\end{figure}
\begin{figure}
    \contcaption{The flowchart shows the sequence of actions triggered by a read access to the DedupBDI cache. Red blocks signify actions happening on the critical path, while green blocks mean actions happening off the critical path. The blue shaded blocks are actions happening off the critical path but only exist in the straightforward implementation and not the final one. Each + sign in any of the blocks signifies an extra latency for tag array access, data array access, or compression}
\end{figure}
\begin{figure}
    \includegraphics[width=\textwidth,height=\textheight]{DedupBDI_Write.pdf}
    \label{fig:DedupBDI_Write}
    \caption[DedupBDI Write]{Write access flowchart.}
\end{figure}
\begin{figure}
    \contcaption{The flowchart shows the sequence of actions triggered by a write access to the DedupBDI cache. All the blocks are shaded in green because any write request should be off the critical path of the processor regardless of it's status in the cache (hit or miss). The blue shaded blocks are actions happening off the critical path but only exist in the straightforward implementation and not the final one. Each + sign in any of the blocks signifies an extra latency for tag array access, data array access, or compression}
\end{figure}

\subsubsection{Tag Miss}
%% TODO: Cite something for the cache inclusion?
We always assume inclusive caches, so a write request can never miss. On a read tag miss, just like a normal cache, a request for the missed address is forwarded to the next cache level. Meanwhile a victim tag line has to be selected according to the tag replacement policy (e.g. LRU) and evicted if necessary. This eviction may or may not trigger a data line eviction, depending on whether the tag was single (no deduplication) or part of a linked list (deduplication), if the tag is part of linked list then the tags before and after it have to be updated (i.e. Have to point to each other instead of pointing to the victim).

Once the requested line is received, it can be used to service the miss request right away, then placement of the new line can start off the critical path. The received line is first hashed and BDI compressed, the hash is used to compare against the existing hash array, if it hits then the received line has to be compared against the line pointed to by the hash entry, this is needed to make sure because the lines could be different but their hashes are similar (i.e. Hash collision). There are four possible outcomes for this scenario, similar to \hyperref[sssec:DedupOperations]{Dedup}:
\begin{itemize}
    \item \textbf{Hash Miss:} No similar hash is found, either because similar lines do not exist in the data array, or because the hash array is not big enough to keep track of all data lines. In this case we use the data replacement policy to evict victim data line(s) until a sufficient area for the compressed line exists. The new tag is inserted and the tag and data lines have to point to each other, the tag will not point to other tags and will not create a linked list because no deduplication is happening yet. A new hash entry will be selected based on the hash replacement policy and will point to the newly inserted data line.
    \item \textbf{Hash Hit, Line Similar:} In which case the received data line can be deduplicated, it will use the same data line and hash entry. Only the new tag needs to be inserted, it's inserted as the head of the linked list and points to the deduplicated data line. The data line's tag counter (deduplication counter) has to be incremented and it has to pint to the new linked list head.
    \item \textbf{Hash Hit, Line Invalid:} Because hashes point to data lines, but data lines do not point to hashes, when a data line is evicted, it's associated hash might not, causing a situation like this to arise. In this case, we utilize the invalid space right away instead of consulting the data replacement policy. This is similar to the same case in \hyperref[sssec:DedupOperations]{Dedup} with one minor difference, if the invalid space is not enough for the compressed line, we use the data replacement policy to start evicting from the current data set until a sufficient area is enough for the compressed data line to be inserted. The tag entry is also inserted, it has to point to the data line, but it doesn't point to any other tags because the data line is not deduplicated yet and shouldn't have a linked list associated with it. The data line in return also has to point to the tag entry, the hash entry is not changed because it already points to the space we used for the data line.
    \item \textbf{Hash Hit, Line Different:} This case happens only on a hash collision, once a collision happens, it's treated like a hash miss with one modification, a new hash insertion is not needed, the same hash entry will be changed to point to the newly inserted data line in only if the line it was previously pointing to was not deduplicated (i.e. Its tag counter is 1), otherwise it remains untouched. This follows the same policy used in \hyperref[sssec:DedupOperations]{Dedup}.
\end{itemize}
\subsubsection{Tag Hit}
A tag hit has to be treated differently depending on whether it's a read or write request, a read hit is simple, after accessing the tag, the tag's data pointer can be used to access the data line and service the request, a write hit however can be complicated because it can cause the data line to change and thus change it's compressed size and/or hash. If the line was distinct (i.e. Had a tag counter of 1) and the new written size is the same or less than the original size then it can be overwritten right away with the tag updated accordingly, otherwise, the tag disconnects from the data line causing its eviction if it's not deduplicated or causing its counter to decrement and then the request is treated as a tag miss and can follow one of the four cases mentioned above.

\subsection{Replacement Policies}
\label{ssec:Replacement Policies}
Some parts of the DedupBDI cache like the tag array can still operate in the same way with the same replacement policies, the tag array can use one of the known replacement policies like LRU. Other parts of the cache have to be treated differently, since the data and hash array are decoupled from the tag array each one of them needs it's own replacement policy.
\subsubsection{Data Array}
\label{sssec:DedupBDIData}
The data array uses a somewhat similar replacement policy to \hyperref[sssec:DedupDataRepl]{Dedup data replacement policy}, a free list keeping track of free data sets (not lines) is used, whenever needed an entry of this list was used to insert a line. If the list if empty then up to four data sets are picked at random, if any of the sets have enough segments for insertion of the line then it's selected right away, otherwise the one with the least sum of deduplication counters is picked. The picked data set then is used for the second part of the replacement policy, in which a segment (or group of segments representing a compressed data line) with the lowest deduplication counter is selected for eviction, segments keep evicted as necessary until a large enough space is freed for the insertion. Each segment or group of segments evicted can trigger a chain of evictions for the tag linked list associated with it.
\subsubsection{Hash Array}
\label{sssec:DedupBDIHash}
The hash array is indexed by a part of the hash, the selection of a victim hash set is thus dependant on the hash that needs to be inserted, once that set is selected, a hash entry is then selected based on the number of deduplication on the data line it point to, if this data line is not deduplicated then this hash can be used and overwritten, otherwise it's not touched. The rationale behind this is to keep hashes pointing to deduplciated lines from getting evicted and causing the cache to miss extra deduplication for a newly incoming line that might not be useful for deduplication.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% Upper Bound
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% TODO: should we discuss "How did we decide what to idealize?". I think we
%% should have a results section for each implementation, to motivate the
%% section after it!
\section{Establishing an Upper Bound}
\label{sec:Upper Bound}
To be able to evaluate the straightforward design we discussed in the previous section, an upper bound must be established, we try to get the upper bound of enhancements by idealizing the design choices we have selected. There are two aspects we can idealize in this design:
\begin{itemize}
    %TODO: Dump data lines, show missed dedup and quantify.
    \item \textbf{Finding a similar line:} Finding a similar line in the DedupBDI cache has two sources of possible error: The first is the hashing itself, because hashing data lines to a smaller space means it will never be perfect and collisions are inevitable. The second limitation is the size of hash array, because the number of entries in the hash array is less than that of the data array, there's a chance that an opportunity for deduplication is missed because the hash array was too small and couldn't keep its hash. Finding a similar line can be idealized by directly searching through the data array and comparing each line until a similar data line is found.
    %TODO: Show figure of data utilization here.
    %TODO: Show L2 Back Invs and use NMRU as ideal instead of LRU?
    \item \textbf{Replacement of data lines:} Imperfections in the replacement policy of data array comes from two sources: the first is the free list, since we cannot make the free list keep track of all free segments in the data array because it would be impractically large, it can only keep track of free sets, that means even if only one segment is used from a set that set is not kept in the free list anymore. The second source is the random selection of data lines whenever the free list is empty, our experiments show that the random selection causes utilization of the data array to be less than a 100\% as shown in \ref{ch:Results}, selecting 4 random sets might now always yield a set with enough free segments, selecting more than 4 can be time inefficient. We idealize the replacement policy by directly searching the whole data cache for free segments that are enough to insert a new compressed line, this ensures a 100\% utilization of the data array, if no free segments are available we use data array wide LRU to pick a victim segment(s) instead of doing it randomly.
\end{itemize}
with the use of perfect deduplication and data insertion, a hash array is no longer needed, and the tag array is no different than a normal cache. The \ref{ch:Results} chapter shows the performance of both the straightforward implementation and the idealized one.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%% Final Implementation
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Final Implementation}
\label{sec:Final Implementation}
In order to improve on the naive implementation and towards the upper bound, we propose the following realistic improvements on the straightforward implementation.
\begin{itemize}
    %TODO: Quantify this
    \item \textbf{Deduplication:} while any realistic deduplication will still depend on the existence of a hash array. The hash array used in the straightforward was the same as \hyperref[sssec:DedupHash]{Dedup}, the hash array was enough to cover deduplications in the data array of \ref{sec:Dedup}, but because of the use of compression in DedupBDI, each line can hold up to eight times the amount of data, the same hash array with the same number of entries might not be enough to cover that amount of deduplication efficiently. In the \ref{ch:Results} we show how varying the number of hash array entries can affect the performance of deduplication in DedupBDI cache.
    \item \textbf{Data free list:} To keep track of all free segments in the data array, a modification to the free list was made, the free list is now divided into multiple free lists, each of which can keep track of data sets with a certain amount of free segments, namely we have eight free lists, the first keeps track of data sets with one free segment and the second with two, until the last which keeps track of data sets with eight or more free segments, whenever a data line needs to be inserted, the free list corresponding to its size is looked up first, if it's empty then the larger ones are searched one by one, if no empty segments exist then we return to the second part of the replacement policy.
    %TODO: Results of L2 back invs.
    %TODO: Need to quantify LRU vs Rand vs NMRU
    %TODO: Quantify how it hurts
    \item \textbf{Replacement of data lines:} Two modifications of the replacement policy were introduced. The first is ignoring the replacement policy in case of a tag miss, hash hit, but invalid data line was found to be hurting the performance as in some cases the space of the invalid line might not be enough but another enough space might exist, the better alternative is to always consult the replacement policy. The second change I cannot write about until I quantify.
\end{itemize}

