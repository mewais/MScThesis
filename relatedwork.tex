%% The following is a directive for TeXShop to indicate the main file
%%!TEX root = diss.tex

\chapter{Related Work}
\label{ch:Related Work}
In this section we describe some of the previous work in cache compression. We discuss how they're different that our picked algorithms.
\section{Special Case Schemes}
\subsection{Zero Content Augmented Cache}
The Zero Content Augmented~\cite{zca} cache takes advantage of the fact that zero lines are very common in today's benchmarks. It uses a special structure called zero content cache to complement normal caches. This zero content cache only saves tags for data lines that are completely null. Special care has to taken to handle cases where a line changes and thus has to be moved from one of the caches to the other one. The choice to use an external zero content cache simplifies the design process. It allows seamless integration with any normal cache without changing its underlying structure.

\section{Data Compression}
\subsection{Frequent Pattern Compression}
\label{ssec:FPC}
Frequent Pattern Compression~\cite{fpc} is one of the earliest cache compression techniques proposed. It's focused on compressing data on an intra-line granularity. If is based on the observation that some patterns are more frequent in data and can be represented by fewer bits. It takes advantage of this observation by dividing each cache line to words. Each word is represented as a 3-bit encoding prefix then a (un)compressed data form. The patterns are frequent enough to achieve good compression, but in the worst case scenario an uncompressed data work will be represented in 35 bits instead of 32. The FPC cache uses tag and data array decoupling, similar to Dedup, to allow more tags than data lines. This simple scheme provides good compression ratios and is not complex to implement.
\subsection{SC2}
The SC2 cache~\cite{sc2} is a statistical compression caches. It uses Huffman-coding~\cite{huffman1952method} to assign variable length codes to data based on their probability of occurance. This allows saving small codes for values with high probability of occurance like zeros for example. While this cache achieves good compression ratios, its downside is that it requires a sampling phase to collect statistics on the frequency of values.
\subsection{HyComp}
The HyComp cache compression~\cite{hycomp} is a hybrid cache compression algorithm. It selects different cache compression algorithm depending on the data type. It first employs a predictor to predict the data type in each line. Then it tries to use the appropriate cache compression for this data type. It uses SC2 for integer compression, BDI for pointer compression, ZCA for zero lines, and it also introduces a new compression technique called FP-H for floating points.

\section{Dictionary Based Compression}
\subsection{CPACK}
CPACK is a combination of dictionary compression and frequent pattern compression (not to be mistaken with the~\ref{ssec:FPC}). It uses encoding for frequent observable patterns and combines this with using dictionaries or partial dictionaries for frequently encountered data values. It maintains dictionaries per data line and is able to compress multiple words in parallel at the same time.
\subsection{Dictionary Sharing}
The Dictionary Sharing cache (DISH) is a dictionary based compression cache. The authors make the observation that there's sufficient value locality in consecutive data lines to merit using dictionaries on a scale bigger than one data line. They use dictionaries on a granularity from one cache line to a superblock (four cache lines). Like other compressed caches it also increases tags over data entries, but it does so using one single tag for a whole superblock.

% \section{Software Compression}
